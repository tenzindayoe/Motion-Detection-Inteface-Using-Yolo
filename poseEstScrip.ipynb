{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ac2fe1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from utils.datasets import letterbox\n",
    "from utils.general import non_max_suppression_kpt\n",
    "from utils.plots import output_to_keypoint, plot_skeleton_kpts\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use(\"TkAgg\")\n",
    "import cv2\n",
    "import numpy as np\n",
    "import socket\n",
    "import json\n",
    "import threading\n",
    "from memory_profiler import profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f2e450",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model():\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    model = torch.load('yolov7-w6-pose.pt', map_location=device)['model']\n",
    "    # Put in inference mode\n",
    "    model.float().eval()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        # half() turns predictions into float16 tensors\n",
    "        # which significantly lowers inference time\n",
    "        model.half().to(device)\n",
    "    return model\n",
    "\n",
    "model = load_model()\n",
    "\n",
    "def saveData(output):\n",
    "    keypoints_dict = {}\n",
    "    output = non_max_suppression_kpt(output, \n",
    "                                     0.25, # Confidence Threshold\n",
    "                                     0.65, # IoU Threshold\n",
    "                                     nc=model.yaml['nc'], # Number of Classes\n",
    "                                     nkpt=model.yaml['nkpt'], # Number of Keypoints\n",
    "                                     kpt_label=True)\n",
    "    with torch.no_grad():\n",
    "        output = output_to_keypoint(output)\n",
    "    for idx in range(output.shape[0]):\n",
    "        kpts = output[idx, 7:].T\n",
    "        print(kpts)  # Print kpts for debugging\n",
    "        keypoints_dict.update({\n",
    "            \"nose\": (kpts[0], kpts[1]),\n",
    "            \"left_eye\": (kpts[2], kpts[3]),\n",
    "            \"right_eye\": (kpts[4], kpts[5]),\n",
    "            \"left_ear\": (kpts[6], kpts[7]),\n",
    "            \"right_ear\": (kpts[8], kpts[9]),\n",
    "            \"left_shoulder\": (kpts[10], kpts[11]),\n",
    "            \"right_shoulder\": (kpts[12], kpts[13]),\n",
    "            \"left_elbow\": (kpts[14], kpts[15]),\n",
    "            \"right_elbow\": (kpts[16], kpts[17]),\n",
    "            \"left_wrist\": (kpts[18], kpts[19]),\n",
    "            \"right_wrist\": (kpts[20], kpts[21]),\n",
    "            \"left_hip\": (kpts[22], kpts[23]),\n",
    "            \"right_hip\": (kpts[24], kpts[25]),\n",
    "            \"left_knee\": (kpts[26], kpts[27]),\n",
    "            \"right_knee\": (kpts[28], kpts[29]),\n",
    "            \"left_ankle\": (kpts[30], kpts[31]),\n",
    "            \"right_ankle\": (kpts[32], kpts[33])\n",
    "        })\n",
    "    return keypoints_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ef302d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(url):\n",
    "    image = cv2.imread(url) # shape: (480, 640, 3)\n",
    "    # Resize and pad image\n",
    "    image = letterbox(image, 960, stride=64, auto=True)[0] # shape: (768, 960, 3)\n",
    "    # Apply transforms\n",
    "    image = transforms.ToTensor()(image) # torch.Size([3, 768, 960])\n",
    "    # Turn image into batch\n",
    "    image = image.unsqueeze(0) # torch.Size([1, 3, 768, 960])\n",
    "    output, _ = model(image) # torch.Size([1, 45900, 57])\n",
    "    return output, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5021cfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_output_singlePicture(output, image):\n",
    "    output = non_max_suppression_kpt(output, \n",
    "                                     0.25, # Confidence Threshold\n",
    "                                     0.65, # IoU Threshold\n",
    "                                     nc=model.yaml['nc'], # Number of Classes\n",
    "                                     nkpt=model.yaml['nkpt'], # Number of Keypoints\n",
    "                                     kpt_label=True)\n",
    "    with torch.no_grad():\n",
    "        output = output_to_keypoint(output)\n",
    "    nimg = image[0].permute(1, 2, 0) * 255\n",
    "    nimg = nimg.cpu().numpy().astype(np.uint8)\n",
    "    nimg = cv2.cvtColor(nimg, cv2.COLOR_RGB2BGR)\n",
    "    for idx in range(output.shape[0]):\n",
    "        plot_skeleton_kpts(nimg, output[idx, 7:].T, 3)\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(nimg)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58a2d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "output, image = run_inference('sample.jpg') #Test image \n",
    "visualize_output_singlePicture(output, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680ab737",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def serverSetup():\n",
    "    # Define the IP address and port number for the server\n",
    "    IP_ADDRESS = \"localhost\"\n",
    "    PORT = 8080\n",
    "    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    server_socket.bind((IP_ADDRESS, PORT))\n",
    "\n",
    "    server_socket.listen(1)\n",
    "    print(\"Listening for incoming connections...\")\n",
    "    client_socket, address = server_socket.accept()\n",
    "    print(f\"Connection established with {address}\")\n",
    "    return client_socket\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe9e475",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_output(frame, output):\n",
    "    output = non_max_suppression_kpt(output, \n",
    "                                     0.25, # Confidence Threshold\n",
    "                                     0.65, # IoU Threshold\n",
    "                                     nc=model.yaml['nc'], # Number of Classes\n",
    "                                     nkpt=model.yaml['nkpt'], # Number of Keypoints\n",
    "                                     kpt_label=True)\n",
    "    with torch.no_grad():\n",
    "        output = output_to_keypoint(output)\n",
    "   \n",
    "    for idx in range(output.shape[0]):\n",
    "        plot_skeleton_kpts(frame, output[idx, 7:].T, 3)\n",
    "        \n",
    "        return  output[0, 7:].T\n",
    "\n",
    "def processKeyPointData(rawData):\n",
    "    #Process the keypoints for Unity Client app.\n",
    "    \n",
    "    #needs to be called every frame\n",
    "    #Todo : Determine ideal frame rate at which this method should be called. \n",
    "    counter = 1 \n",
    "    x = []\n",
    "    y = [] \n",
    "    confidence = []\n",
    "    names = [\"nose\",\"left_eye\",\"right_eye\",\"left_ear\",\"right_ear\",\"left_shoulder\",\"right_shoulder\",\"left_elbow\",\"right_elbow\",\"left_wrist\",\"right_wrist\",\"left_hip\",\"right_hip\",\"left_knee\",\"right_knee\",\"left_ankle\",\"right_ankle\"]\n",
    "    for obj in range(len(rawData)):\n",
    "        if counter==1:\n",
    "            x.append(rawData[obj])\n",
    "        elif counter ==2 : \n",
    "            y.append(rawData[obj])\n",
    "        elif counter ==3 : \n",
    "            confidence.append(rawData[obj])\n",
    "            counter = 0\n",
    "        counter+= 1\n",
    "    resultDictionary = {}\n",
    "    for c in range(len(confidence)):\n",
    "        if (confidence[c]>0.8):\n",
    "#             resultX.append(x[c])\n",
    "#             resultY.append(y[c])\n",
    "#             resultLabel.append(names[c])\n",
    "            resultDictionary[names[c]] = [x[c],y[c]]\n",
    "    return resultDictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa94f279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Live cam pose detection and keypoint extraction \n",
    "\n",
    "def visualize_output(frame, output):\n",
    "    output = non_max_suppression_kpt(output, \n",
    "                                     0.25, # Confidence Threshold\n",
    "                                     0.65, # IoU Threshold\n",
    "                                     nc=model.yaml['nc'], # Number of Classes\n",
    "                                     nkpt=model.yaml['nkpt'], # Number of Keypoints\n",
    "                                     kpt_label=True)\n",
    "    with torch.no_grad():\n",
    "        output = output_to_keypoint(output)\n",
    "   \n",
    "    for idx in range(output.shape[0]):\n",
    "        plot_skeleton_kpts(frame, output[idx, 7:].T, 3)\n",
    "        \n",
    "        return  output[0, 7:].T\n",
    "\n",
    "#Yolov7 model loaded \n",
    "model = load_model()\n",
    "# WEbcame Video Capture (Current resolution = 100x100. Higher res than this causes frame rate issues****)\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 128)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 128)\n",
    "cv2.namedWindow(\"Pose Estimation\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "\n",
    "\n",
    "# Window resize = 950 x 950\n",
    "cv2.resizeWindow(\"Pose Estimation\", 950, 950)\n",
    "\n",
    "\n",
    "tempDict = 0 \n",
    "client_socket = serverSetup()\n",
    "counter = 0 ;\n",
    "while True:\n",
    "    # Read a frame from the video stream\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Resize and pad image\n",
    "    image = letterbox(frame, 128, stride=64, auto=True)[0]\n",
    "    # Apply transforms\n",
    "    image = transforms.ToTensor()(image)\n",
    "    \n",
    "    # Turn image into batch\n",
    "    image = image.unsqueeze(0)\n",
    "    # Make predictions on the image batch\n",
    "    output, _ = model(image)\n",
    "\n",
    "    # Visualize the detected keypoints on the video frame\n",
    "    tempDict = visualize_output(frame, output)\n",
    "    if counter%2 ==0:\n",
    "        try:\n",
    "            tempDict = processKeyPointData(tempDict)\n",
    "            joint_positions_json = json.dumps(tempDict)\n",
    "            \n",
    "            # Send message to server in a new thread\n",
    "    #         threading.Thread(target=send_message, args=(client_socket, joint_positions_json)).start()\n",
    "            client_socket.sendall((joint_positions_json + \"\\n\").encode())\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        counter =0\n",
    "    counter += 1\n",
    "\n",
    "    # Video with keypoints. \n",
    "    cv2.imshow('Pose Estimation', frame)\n",
    "\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        client_socket.close()\n",
    "        break\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3077f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = load_model()\n",
    "# WEbcame Video Capture (Current resolution = 100x100. Higher res than this causes frame rate issues****)\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 60)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 60)\n",
    "cv2.namedWindow(\"Pose Estimation\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "# Window resize = 950 x 950\n",
    "cv2.resizeWindow(\"Pose Estimation\", 950, 950)\n",
    "\n",
    "tempDict = 0 \n",
    "client_socket = serverSetup()\n",
    "counter = 0 \n",
    "frame_num = 0\n",
    "while True:\n",
    "    # Read a frame from the video stream\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    frame_num += 1\n",
    "    \n",
    "    if frame_num % 2 != 0:\n",
    "        continue\n",
    "    \n",
    "    # Resize and pad image\n",
    "    image = cv2.resize(frame, (60, 60))\n",
    "    image = transforms.ToTensor()(image)\n",
    "    image = image.unsqueeze(0)\n",
    "    \n",
    "    # Make predictions on the image batch\n",
    "    output, _ = model(image)\n",
    "\n",
    "    # Visualize the detected keypoints on the video frame\n",
    "    tempDict = visualize_output(frame, output)\n",
    "    \n",
    "    try:\n",
    "        tempDict = processKeyPointData(tempDict)\n",
    "        joint_positions_json = json.dumps(tempDict)\n",
    "        \n",
    "        # Send message to server\n",
    "        client_socket.sendall((joint_positions_json + \"\\n\").encode())\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Video with keypoints. \n",
    "    cv2.imshow('Pose Estimation', frame)\n",
    "    tempDict = {}\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        client_socket.close()\n",
    "        break\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de333d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing Keyframes \n",
    "t = processKeyPointData(tempDict)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.gca().invert_yaxis()\n",
    "plt.scatter(t[1],t[2])\n",
    "\n",
    "\n",
    "for i, name in enumerate(t[0]):\n",
    "    plt.annotate(name, (t[1][i], t[2][i]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
